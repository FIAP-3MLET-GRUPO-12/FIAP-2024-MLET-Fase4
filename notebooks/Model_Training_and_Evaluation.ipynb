{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Tesla Stock Price Prediction - Day 2: Model Training and Evaluation\n",
        "\n",
        "This notebook focuses on building, training, and evaluating our LSTM model for Tesla stock price prediction. We'll cover:\n",
        "\n",
        "1. Loading and preparing the preprocessed data\n",
        "2. Building the LSTM model architecture\n",
        "3. Training the model with early stopping\n",
        "4. Model evaluation and performance metrics\n",
        "5. Visualizing predictions vs actual prices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the preprocessed data from Day 1\n",
        "# Note: You need to run Day 1 notebook first to have these variables available\n",
        "# or load them from saved files\n",
        "\n",
        "# Build the LSTM model with multiple layers\n",
        "def create_lstm_model(sequence_length, n_features):\n",
        "    model = Sequential([\n",
        "        # First LSTM layer with return_sequences=True to connect to the next LSTM layer\n",
        "        LSTM(units=50, return_sequences=True, input_shape=(sequence_length, n_features)),\n",
        "        Dropout(0.2),\n",
        "        # Second LSTM layer\n",
        "        LSTM(units=50),\n",
        "        Dropout(0.2),\n",
        "        # Output layer\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "sequence_length = 60  # Same as in Day 1\n",
        "n_features = 5  # Number of features (Open, High, Low, Close, Volume)\n",
        "model = create_lstm_model(sequence_length, n_features)\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss During Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Save the model and training history\n",
        "model.save('models/tesla_lstm_model.keras')\n",
        "\n",
        "# Save training history to a file\n",
        "history_dict = history.history\n",
        "np.save('models/training_history.npy', history_dict)\n",
        "\n",
        "# You can load the history later with:\n",
        "# history_dict = np.load('models/training_history.npy', allow_pickle=True).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform predictions and actual values to original scale\n",
        "# We need to reshape the data back to the original format for inverse transform\n",
        "def inverse_transform_predictions(scaler, y_pred, y_test):\n",
        "    # Create a dummy array with the same shape as our original data\n",
        "    dummy = np.zeros((len(y_pred), 5))\n",
        "    # Put our predictions in the Close price column (index 3)\n",
        "    dummy[:, 3] = y_pred.flatten()\n",
        "    # Inverse transform\n",
        "    y_pred_transformed = scaler.inverse_transform(dummy)[:, 3]\n",
        "    \n",
        "    # Do the same for actual values\n",
        "    dummy[:, 3] = y_test.flatten()\n",
        "    y_test_transformed = scaler.inverse_transform(dummy)[:, 3]\n",
        "    \n",
        "    return y_pred_transformed, y_test_transformed\n",
        "\n",
        "# Transform predictions back to original scale\n",
        "y_pred_orig, y_test_orig = inverse_transform_predictions(scaler, y_pred, y_test)\n",
        "\n",
        "# Calculate metrics\n",
        "mae = mean_absolute_error(y_test_orig, y_pred_orig)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_orig))\n",
        "mape = mean_absolute_percentage_error(y_test_orig, y_pred_orig)\n",
        "\n",
        "print(f'Mean Absolute Error: ${mae:.2f}')\n",
        "print(f'Root Mean Squared Error: ${rmse:.2f}')\n",
        "print(f'Mean Absolute Percentage Error: {mape*100:.2f}%')\n",
        "\n",
        "# Plot predictions vs actual values\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(y_test_orig, label='Actual Prices', alpha=0.8)\n",
        "plt.plot(y_pred_orig, label='Predicted Prices', alpha=0.8)\n",
        "plt.title('Tesla Stock Price Prediction vs Actual')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Price (USD)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot prediction error distribution\n",
        "errors = y_test_orig - y_pred_orig\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(errors, bins=50)\n",
        "plt.title('Distribution of Prediction Errors')\n",
        "plt.xlabel('Error (USD)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=errors)\n",
        "plt.title('Box Plot of Prediction Errors')\n",
        "plt.ylabel('Error (USD)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Model Performance and Possible Improvements\n",
        "\n",
        "Our current model configuration:\n",
        "- Window size: 60 days\n",
        "- LSTM layers: 2 (50 units each)\n",
        "- Dropout rate: 0.2\n",
        "- Batch size: 32\n",
        "- Maximum epochs: 50 (with early stopping)\n",
        "\n",
        "Possible improvements:\n",
        "1. **Architecture Adjustments**:\n",
        "   - Increase/decrease number of LSTM units\n",
        "   - Add more LSTM layers\n",
        "   - Adjust dropout rates\n",
        "   - Try bidirectional LSTM layers\n",
        "\n",
        "2. **Training Parameters**:\n",
        "   - Experiment with different batch sizes\n",
        "   - Adjust learning rate\n",
        "   - Try different optimizers (RMSprop, SGD with momentum)\n",
        "   - Modify early stopping patience\n",
        "\n",
        "3. **Feature Engineering**:\n",
        "   - Include technical indicators (RSI, MACD, etc.)\n",
        "   - Add market sentiment data\n",
        "   - Consider different window sizes\n",
        "\n",
        "4. **Data Preprocessing**:\n",
        "   - Try different normalization techniques\n",
        "   - Experiment with data augmentation\n",
        "   - Include more historical data\n",
        "\n",
        "To implement any of these improvements, modify the corresponding parameters in the notebook cells above and retrain the model.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
